\documentclass[pmlr]{jmlr}
\input{_head}
%TODO: Max 14 pages
%TODO: No personal information or reference to the authors should be introduced in the submitted paper.
%TODO: Submissions will be evaluated concerning their technical quality, relevance, significance, originality and clarity. 

\title
[Homogenous ensemble of undersampled majority class]
{
	Homogenous ensemble of undersampled majority class for highly imbalanced data binary classification%\titlebreak
}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}
\begin{keywords}
classification, classifier ensemble, undersampling, imbalanced data
\end{keywords}

%%%
%%% Introduction
%%%
\section{Introduction}
\label{sec:intro}

%Most of the classification algorithms assume that there are no significant disproportions among instances from different classes. Nevertheless, in many practical tasks, we may observe that instances from one class (so-called \textit{majority class}) significantly outnumber the objects from remaining classes (\textit{minority class}). Most of traditional classifiers have a bias in favor of the majority class although more often the minority class is more interesting, because misidentification of an instance belonging to it is usually much more expensive than assigning an instance from majority class to minority one. A good example is an undetected fraud that would be more expensive than the cost of additional analysis of a correct transaction classified as fraudless transaction. Such a problem is known as imbalanced data classification \cite{Sun:2009,Wang:2017}, where an unequal number of instances from the examined classes plays a key role during the classifier learning. Various approaches have been proposed in the literature to tackle this challenging difficulty embedded in the nature of data. Usually, the researchers are focusing on maximizing the correct minority class classification. At the same time, performance on the majority class cannot be neglected.

%\begin{figure}[!h]
%\floatconts
%  {fig:east}
%  {\caption{Easy separable imbalance dataset}}
%  {\includegraphics[clip=true, trim = 11 11 11 11, width=0.25\linewidth]{figures/easy}}
%\end{figure}

%In this project we will focus on binary imbalanced problems, because this setup is the one most frequently studied in the literature and most commonly meet in practical problems, e.g.,\textit{fault detection} or \textit{spam filtering}. Therefore, another important issue is proposing an appropriate quality measure that would be adequate for imbalanced data classification \cite{Elazmeh:2006}.

%In case of imbalanced data classification the disproportion between the different classes is not the sole issue of learning difficulties. One may easily came up with an example where the instance distributions from different classes are well-separated, as depicted in Fig.\ref{fig:easy}.

%Proposing a efficient classifier for such a task is not a challenge. Unfortunately, instances from the minority class often form clusters of an unknown structure that are scattered \cite{Napierala:2012}. Additional complication comes from the fact that during learning, the number of intactness from the minority class may be not sufficient enough for the learning algorithm to acquire the appropriate generalization level, which in effect can cause \emph{overfitting} \cite{Chen:2008}. All those problems are a focus of intense research \cite{Chawla:2002,Bunkhumpornpat:2009,Kubat:1997}.

%Methods for imbalanced data classification can be divided into three main groups \cite{Lopez:2012}.

%\begin{figure}[!h]
%\floatconts
%  {fig:subfigex}
%  {\caption{Examples of data preprocessing methods.}}
%  {%
%    \subfigure[Original dataset]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 11 141 141 11]
%    {figures/preprocessing}}%
%    \qquad
%    \subfigure[\textsc{smote}]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 141 141 11 11]
%    {figures/preprocessing}}%
       
%    \subfigure[\textsc{adasyn}]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 11 11 141 141]
%    {figures/preprocessing}}%
%    \qquad
%    \subfigure[Undersampling]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 141 11 11 141]
%    {figures/preprocessing}}%
%  }
%\end{figure}

%\noindent\textbf{ Data preprocessing methods}. This approach focuses on reducing the number of objects in majority class (\emph{undersampling}) or generating new objects of the minority class (\textit{oversampling}). The difference between \emph{under-} and \emph{oversampling} is presented in Fig. \ref{fig:over-under}.

%These mechanisms have the objective of balancing the quantity of instances from considered classes. For oversampling, new instances are random copies of existing ones or are generated in a guided manner. The most popular method is \textsc{smote} \cite{Cha2002} algorithm, which creates new instances on a basis of existing ones by slightly modifying the values of their attributes. As a result, new artificial examples that are in compliance with the minority class distribution are generated.  Other oversampling methods are \textsc{adasyn} \cite{He:2008}, in which a difficulty of an object for the classifying model is considered or \textsc{ramob}oost \cite{Chen:2010}. Unfortunately, methods such as \textsc{smote} may lead to changes in the characteristic of the minority class and in result to \emph{overfitting} the classifier, what was shown in Fig. \ref{fig:wrong_smote}.

%\begin{figure}[htbp]
%\floatconts
%  {fig:subfigex}
%  {\caption{Example of wrong \textsc{smote} oversampling.}}
%  {%
%    \subfigure[Original dataset]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 11 11 141 11]
%    {figures/wrong_smote}}%
%    \qquad
%    \subfigure[\textsc{smote}]{\label{fig:circle}%
%    \includegraphics[clip=true, width=.25\linewidth,
%    				 trim = 141 11 11 11]
%    {figures/wrong_smote}}%
%  }
%\end{figure}

%W WYPADKU UNDERSAMPLINGU NIE WYSTEPUJE RYZYKO NIESŁUSZNEGO ROZSZERZENIA PRZESTRZENI WZORCÓW KLASY MNIEJSZOŚCIOWEJ, CO MA MIEJSCE W CHOĆBY SMOTE.

%Several modifications of \textsc{smote} have been proposed that are able to identify the instances to be copied in a more intelligent fashion such as \emph{Borderline}\textsc{smote} \cite{Han:2005}. It generates new instances from the minority class close to the decision border. \emph{Safe-Level} \textsc{smote} \citep{Bunkhumpornpat:2009} and \textsc{ln}-\textsc{smote} \cite{Maciejewski:2011} reduce the probability of generating synthetic instances of the minority class in areas where the predominant objects are that of the majority class. It is worth noticing that our team proposed two novel solutions to this problem: \textsc{rbo} \cite{koziarski2017radial} and \textsc{ccr} that enforce instances from the majority-class to be relocated from the areas where the minority-class instances are present \cite{Koziarski:2017amcs}. Methods of \emph{undersampling} are built around the idea of randomly removing the instances from the majority-class or removing them from the areas in such way that the  quality of the classifier is not disrupted using neighbor analysis.%\todo{W opisie metod przetwarzania wstępnego skupiamy się chyba wyłącznie na \emph{oversamplingu}.}\todo{Nie uwzględniłem czystego losowego \emph{oversamplingu}, bo skoro powtarza istniejące już obiekty, to ilustracja byłaby bezsensowna.}

%\begin{figure}[hbt]
%\centering
%\includegraphics[width=.9\linewidth]{figures/smote}
%\caption{Example of oversampling using \textsc{smote}}
%\end{figure}

%\noindent\textbf{ Inbuilt mechanisms.} In this approach existing classification algorithms are adapted for imbalanced problems ensuring balanced accuracy for instances from both classes. Two of the most popular areas of research of this methods are using one-class classification  \cite{Japkowicz:1995}, usually known as learning without counterexamples, where the goal is to learn the minority class decision areas and because of the frequently assumed regular, closed shape of the decision borders is adequate to the clusters created by minority classes \cite{Krawczyk:2014ins}. The disproportion between the number of instances in classes is then omitted. Another approach is the (\textit{cost sensitive}) classification, where the algorithm takes into account the asymmetrical loss function that assigns a higher cost to a misclassification of an instance form a minority class \cite{Krawczyk:2014,Lopez:2012,He:2009,Zhou:2006}. Unfortunately such methods can cause a reverse bias towards the minority class. 

%Worth noting are methods based on ensemble classification \cite{Wozniak:2014}, like \emph{\textsc{smote}Boost} \cite{Chawla:2003} and \emph{AdaBoost.NC} \cite{Wang:2010}%, or \emph{Multi-objective Genetic Programing Ensemble} \cite{Bhowan:2013}.

%\noindent\textbf{Hybrid methods.} They combine the advantages of methods using data pre-processing with the classification methods. The most popular category is the hybridisation of \emph{under-} and \emph{oversampling} with ensemble classifiers \cite{Galar:2012}. This approach allows the data to be independently  processed for each of the base model. Algorithms formed on modifications of \emph{Bagging} and \emph{Boosting} \cite{Chawla:2003} enjoy wide popularity.

The main contributions of this work are:
\begin{itemize}
	\item metoda konstrukcji komitetu na k-fold,
	\item propozycje reguł decyzyjnych,
	\item metoda pruningu dostosowującego regułę decyzyjną do zbioru testowego
	\item implementacja i ewaluacja eksperymentalna
\end{itemize}

\vspace{5em}
%%%
%%% Method
%%%
\section{Homogenous ensemble based on undersampling the majority class}
\label{sec:intro}
\subsection{Establishing ensemble}

Complex oversampling methods, such as \textsc{smote} or \textsc{adasyn}, despite the large possibilities in most of the imbalanced problems, are not applicable to extreme situations where the minority class is represented by only a few samples, which makes it impossible to designate the nearest neighbors to create a new synthetic object. This could lead to the use of \emph{undersampling} in such problems, but it is characterized, due to high randomness, by a strong instability in a situation of high \textsc{ir} (\emph{imbalance ratio}), which does not allow for the development of a reliable solution.

A popular answer to the above-mentioned problem are the ensemble methods of \emph{Bagging} or \emph{Boosting}, characterized by random sampling with replacement of the training set, breaking a large problem, into a set of smaller problems. This work proposes a basic method, which also breaks the imbalanced task, but with ensuring the use of all the patterns available in the data set, but without a risk of overlapping. Its description can be found in Algorithm 1.

\begin{algorithm}[!h]
\caption{Training classifier ensemble from multiple balanced training datasets separated from one imbalanced dataset of binary problem}\label{alg:moore}
Given a dataset $DS$:
\begin{enumerate*}
	\item Divide $DS$ into subsets of minority- $MinC$ and majority-class $MajC$
	\item Calculate imbalanced ratio $IR$ as the proportion of the number of patterns in $MinC$ and $MajC$ 
	\item Establish $k$ by rounding $IR$ to nearest integer
	\item Perform a \emph{shuffled k-fold division} of $MajC$ to produce a set of subsets $MajC_1, MajC_2, \ldots, MajC_k$ 
	\item For every $i$ in range to $k$
	\begin{enumerate*}
		\item Join $MajC_i$ with $MinC$ to prepare a training set $TS_i$,
		\item Train classifier $\Psi_i$ on $TS_i$ and add it into ensemble
	\end{enumerate*}
\end{enumerate*}
\end{algorithm}

After dividing the dataset with imbalanced binary problem into separated minority ($MinC$) and majority class ($MajC$), we are calculating the \textsc{ir} (\emph{imbalanced ratio}) between given classes. Rounding \textsc{ir} to the nearest integer value $k$ allows us to find the optimal division coefficient of the majority class samples in the context of maximizing the balance between the $MinC$ and any $MajC_i$ subsets while ensuring that all $MajC$ patterns are used in learning process with no overlapping between the individual $MajC_i$'s. Each of $k$ classifiers $\Psi_i$ is trained on union of $MinC$ and $MajC_i$ sets.

\paragraph{Extending pool with oversampling} As an extension of the method of classifier ensemble construction, it is also proposed to extend its pool by a model learned on an additional data set, which is a full set of data subjected to \emph{oversampling}. It is worth testing if the knowledge gained from this method may be a valuable contribution to the ensemble decision. Due to impossibility to use \textsc{smote} or \textsc{adasyn} for oversampling the minority class with only few instances, only its basic variant will be used. 

%%% Fuser
\subsection{Fuser design}

In addition to ensuring the diversity of the classifiers pool, which we achieve by a homogenous committee built on disjoint subsets of the majority class supplemented by minority patterns, the key aspect of the hybrid classification system is the appropriate design of its \emph{fuser} -- the element responsible for making decisions based on the answers of the base classifiers.

There are two groups of solutions here. The first are based on component \emph{decisions} of the committee, most often employing the \emph{majority voting} to produce a final decision. The decision rules proposed in this work are, however, part of the second group, where the \emph{fuser} is carried out by \emph{averaging} (or \emph{accumulating}) the \emph{support vectors} received from the members of a pool.

\begin{note}
It should be remembered that in such methods, it is necessary to use a \emph{probabilistic classification model}, which also requires \emph{quantitative} and not \emph{qualitative data}.
\end{note}


Five fusers were proposed:

\begin{enumerate}
	\item \textbf{\textsc{reg}} --- regular accumulation of support, without weighing the members of the committee.
	\item \textbf{\textsc{wei}} --- accumulation weighted after members of the committee.

The weight of the classifier in the pool is its quality achieved for the training set. We can not use here the measure of \emph{accuracy}, which does not fit with the task of the imbalanced classification, so we decided on a \emph{balanced accuracy} \citep{brodersen2010balanced}.

	\item \textbf{\textsc{nor}} --- same as \textbf{\textsc{wei}}, but with normalization of weights,

Aby premiować klasyfikatory o wyższej mocy dyskryminacyjnej, wagi są poddawane normalizacji standardowej
	
	\item - con akumulacja ważona po wzorcach, przez kontrast,
	\item - nci iloczyn znormalizowanych wag i kontrastu
\end{enumerate}



Wyliczanie wag. Accuracy się nie sprawdzi, więc BAC.

Jeśli klasyfikujemy nie jeden wzorzec, a wiele, wagi mogą być też dla pojedynczych próbek, dla podbicia, a więc pojawia się KONTRAST. Mamy takie ładne ilustracje z badań, dodajmy rysunek chociaż jeden poglądowo.

\begin{figure}[!h]
\floatconts
  {fig:subfigex}
  {\caption{Rysunek.}}
  {%
    \subfigure[O jaki ładny]{\label{fig:circle}%
    \includegraphics[width=.5\linewidth, trim = 0 100 0 100,clip=true]
    {figures/ecoli-0-1_vs_2-3-5}}%
    \subfigure[A ten brzydki]{\label{fig:circle}%
    \includegraphics[width=.5\linewidth, trim = 0 100 0 100,clip=true]
    {figures/ecoli-0-3-4-6_vs_5}}%
  }
\end{figure}

Potencjał kontrastu dla danych strumieniowych.

Proponowane metody decyzyjne.

W konstrukcji reguły decyzyjnej opieramy się na wsparciu dla klasy pozytywnej.


Duża skala niezbalansowania to duża wielkość komitetu (ilustracja zależności na wykresie). Przyda się więc przycinanie (pruning).

%%% Pruning
\subsection{Ensemble pruning}

Wyjaśnienie podejścia do pruningu. Wyliczamy wzajemną zależność statystyczną (Wilcoxonem) pomiędzy wsparciami członków i grupujemy -- omijając kwestię 1z2 2z3 ale nie 1z3 -- je uśredniając wsparcia w obrębie grupy. Uśredniamy też wagi i tworzymy tak dwupoziomowy system fuzji (potrzebna ilustracja).

Pruning też jest w kontekście klasyfikacji wielu wzorców na raz.

Wyjaśnienie kwestii wspomnianej wcześniej i uzasadnienie pominięcia jej analizy. 


\begin{figure}[!h]
\floatconts
  {fig:teximage}
  {\caption{Scheme of using k-Fold division in ensemble construction}}
  {\input{figures/ensemble_scheme}}
\end{figure}

%%%
%%% Introduction
%%%
\section{Experiment design}
\label{sec:intro}

For the experimental evaluation of the proposed method, a collection of datasets made available with \textsc{keel} \citep{alcala2011keel} was used, focusing on a section containing highly unbalanced data, with \textsc{ir} greater than 9 \citep{fernandez2009hierarchical}. From among the available datasets, \oldstylenums{40} were selected presenting only binary problems with quantitative attributes. A review of selected datasets, including information on their number of features, the number of patterns in each class and the unbalance ratio is presented in Table 1.

\begin{table}[!h]
\footnotesize
\centering
\setlength{\tabcolsep}{3.5pt}
\def\arraystretch{1}
\begin{tabular}{@{}|rl|c|ccc|c|}\hline%

\multirow{2}{*}{\bfseries \#} &
\multirow{2}{*}{\bfseries Dataset} &
\multirow{2}{*}{\bfseries Features} &
\multicolumn{3}{c|}{\bfseries Samples} &
\multirow{2}{*}{\bfseries \textsc{ir}} 	
\\

& & & 
\multicolumn{1}{c}{\textsc{all}} & 
\multicolumn{1}{c}{\textsc{maj}} & 
\multicolumn{1}{c|}{\textsc{min}} &

	\\\hline\hline
	
	\csvreader[head to column names,
	           late after line=\csvifoddrow{\\}{\\\rowcolor{gray!10!white}},
	           late after last line = \\\hline]
	{results/datasets.csv}{}%
	{
	
	\idx & \multicolumn{1}{l|}{\emph{\dbname}} & \features & 
	\multicolumn{1}{c}{\samples} & 
	\multicolumn{1}{c}{\majority} & 
	\multicolumn{1}{c|}{\minority} & \ir
	
	}%
\end{tabular}
\caption{Summary of imbalanced datasets chosen for evaluation}
\end{table}


As may be observed in the summary, the experiments are based on datasets with relatively small spatiality (up to \oldstylenums{13} dimensions), with imbalance ratio from \oldstylenums9 to even \oldstylenums{40}. The datasets provided by \textsc{keel}, to ensure easy comparison between results presented in various research, are already pre-divided into five parts, which forces the use of \emph{k-fold cross-validation} with $k = 5$ in experiments \citep{alpaydin2009introduction}.

In the task of imbalanced data classification, due to its strong bias towards majority class, the \emph{accuracy} measure is not a proper tool. For a reliable result, a measure of \emph{balanced accuracy} is given as test results.

Both the implementation of the proposed method and the experimental environment have been constructed using the \emph{scikit-learn} library \citep{scikit-learn} in version \emph{0.20.dev0}\footnote{At the time of conducting research, only the development version of the package already has the implementation of \emph{balanced accuracy} measure.}. Among the available classification models, the \textsc{mlp} (\emph{Multilayer Perceptron}) and \textsc{svc} (\emph{Support Vector Machine}) were rejected. First one was not able to build a correct model due to the lack of convergence on the small datasets (minority class of data chosen for experiments is often represented by only two patterns in cross-validated folds) and second, whose probabilistic interpretation is measurable only with sufficiently large data sets, did not allow credible construction of a fuser. As base classifiers, the following algorithms were used:

\begin{itemize}
	\item \emph{Gaussian Naive Bayes} (\textsc{gnb}) \citep{gnb},
	\item \emph{k-Nearest Neighbors} (k\textsc{nn}) --- with \oldstylenums{5} neighbors and \emph{Minkowski} metric,
	\item \emph{Decision Tree Classifier} (\textsc{dtc}) --- with \emph{Gini} criterion \citep{loh2011classification}.
\end{itemize}

To provide a comparative result for the method presented in the following paper, each base classifier was also tested for the raw, imbalanced dataset and its under- and oversampled versions. Undersampling, due to high instability of results, was repeated five times on each fold. Used statistical analysis tool was a paired dependency between the classifier, which achieved the highest result and each of the others, calculated using the signed-rank \emph{Wilcoxon} test \citep{wilcoxon1945individual}.

Pełną implementację zaproponowanej metody i skrypt umożliwiający powtórzenie zaprezentowanych badań można odnaleźć w repozytorium abc.

The full implementation of the proposed method and the script allowing the repetition of the presented research may be found in the git repository available at \url{url-removed-due-to-} \url{blind-review}.

%%%
%%% Introduction
%%%
\section{Experimental evaluation}
\label{sec:intro}

Przedstawienie tabel.

\begin{sidewaystable}
	\restable{GNB}{\textsc{gnb}}
\end{sidewaystable}

\begin{sidewaystable}
	\restable{kNN}{k\textsc{nn}}
\end{sidewaystable}

\begin{sidewaystable}
	\restable{DTC}{\textsc{dtc}}
\end{sidewaystable}

Tabela zbiorcza zwycięstw w zależności od parametrów (z grupowaniem).

Interpretacja wyników, czyli co zostało należycie uprawdopodobnione.

%%%
%%% Conclusions
%%%
\section{Conclusions}
\label{sec:intro}
Co zostało zaproponowane.

Na co pozwala taka metoda.

Do jakich rezultatów doprowadziła.

Jakie są plany na przyszłość (czyli co robisz w wakacje).



%%%
%%% Acknowledgements and references
%%%
\acks{
	Acknowledgements go here.
}
\bibliography{bibliography}

\end{document}
